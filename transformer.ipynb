{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, re \n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "We represent each word with an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim):\n",
    "    super(Embedding, self).__init__()\n",
    "    self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.embed(x)\n",
    "    return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "- $pos$ is the position of the token in the sequence.\n",
    "- $i$ is the dimension.\n",
    "- $d_{\\text{model}}$ is the dimension of the embeddings.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "The intuition for me is that positional encoding uses this function to give the model some information as to the relative position of a word in a sequence.\n",
    "\n",
    "Suppose we had the sequence \"I like dogs,\" where A, B, and C are the embeddings for each word respectively. Let's assume that our embeddings have only five dimensions (an unrealistic but simple example.)\n",
    "\n",
    "\"I\": $A = [A_1, A_2, A_3, A_4, A_5]$\n",
    "\n",
    "\"like\": $B = [B_1, B_2, B_3, B_4, B_5]$\n",
    "\n",
    "\"dogs\": $A = [C_1, C_2, C_3, C_4, C_5]$\n",
    "\n",
    "For each embedding, we create a positional encoding vector using the following formula:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The positional encoding uses sine and cosine functions of different frequencies.\n",
    "- For even dimensions, sine function is used.\n",
    "- For odd dimensions, cosine function is used.\n",
    "- The denominator $10000^{\\frac{2i}{d_{\\text{model}}}}$ ensures that the positional encodings for different dimensions have different frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
